{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nTo save code to GitHub:\\nCommand Prompt -> Make sure in correct folder - cd Coding Lessons\\ngit add gpt_api.ipynb\\ngit commit -m \"Whatever change made\"\\ngit push\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "import json\n",
    "json_file = open(\"key.json\")\n",
    "json_data = json.load(json_file)\n",
    "api_key = json_data[\"key\"]\n",
    "openai.api_key = api_key\n",
    "\n",
    "''' \n",
    "To save code to GitHub:\n",
    "Command Prompt -> Make sure in correct folder - cd Coding Lessons\n",
    "git add gpt_api.ipynb\n",
    "git commit -m \"Whatever change made\"\n",
    "git push\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28202\n",
      "29202\n",
      "30202\n",
      "31202\n",
      "32202\n",
      "33202\n"
     ]
    }
   ],
   "source": [
    "def pureWebcam():\n",
    "    vid = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "    frameNumber = 30\n",
    "   \n",
    "   \n",
    "    papapy = 0\n",
    "    cv2.imshow(\"Tester\", vid.read()[1])  \n",
    "    def mouse_callback(event, x, y, flags, params):\n",
    "        global papapy\n",
    "        if(event == cv2.EVENT_LBUTTONDOWN):\n",
    "            print(papapy)\n",
    "            papapy += 1000\n",
    "    cv2.setMouseCallback('Tester', mouse_callback)\n",
    "\n",
    "\n",
    "    while True:\n",
    "        # Get current frame from webcam - vid.read() returns 2 things, _, is used for the things we don't care about\n",
    "        _, frame = vid.read()\n",
    "        cv2.putText(frame, str(papapy), (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (255, 0, 0), 2)\n",
    "        papapy += 1\n",
    "        cv2.imshow(\"Tester\", frame)\n",
    "\n",
    "\n",
    "        # Press q to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "pureWebcam()\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordAudio():\n",
    "    duration = 5 # seconds\n",
    "    sample_rate = 44100\n",
    "    input = 5 # microphone\n",
    "    output = 8 # audio device\n",
    "\n",
    "    print(\"Recording\")\n",
    "    myrecording = sd.rec(int(duration * sample_rate), samplerate = sample_rate, channels = 2)\n",
    "    sd.wait()\n",
    "    print(\"Not Recording\")\n",
    "\n",
    "    write('recording.wav', sample_rate, myrecording)\n",
    "    audio_file = open(\"recording.wav\", \"rb\")\n",
    "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "\n",
    "    return transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myMessage' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\brody\\Coding Lessons\\gpt_api.ipynb Cell 4\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brody/Coding%20Lessons/gpt_api.ipynb#W4sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     vid\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brody/Coding%20Lessons/gpt_api.ipynb#W4sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/brody/Coding%20Lessons/gpt_api.ipynb#W4sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m openWebcamWindow()\n",
      "\u001b[1;32mc:\\Users\\brody\\Coding Lessons\\gpt_api.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brody/Coding%20Lessons/gpt_api.ipynb#W4sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     cv2\u001b[39m.\u001b[39mrectangle(frame, (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m), (\u001b[39m100\u001b[39m, \u001b[39m100\u001b[39m), greenColor, thickness)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brody/Coding%20Lessons/gpt_api.ipynb#W4sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     cv2\u001b[39m.\u001b[39mrectangle(frame, (\u001b[39m120\u001b[39m, \u001b[39m0\u001b[39m), (\u001b[39m220\u001b[39m, \u001b[39m100\u001b[39m), greenColor, thickness)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/brody/Coding%20Lessons/gpt_api.ipynb#W4sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m cv2\u001b[39m.\u001b[39mputText(frame, myMessage, (\u001b[39m250\u001b[39m, \u001b[39m75\u001b[39m), font, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m, redColor, \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brody/Coding%20Lessons/gpt_api.ipynb#W4sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m cv2\u001b[39m.\u001b[39mputText(frame, chatGPTMessage, (\u001b[39m250\u001b[39m, \u001b[39m125\u001b[39m), font, \u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m, redColor, \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/brody/Coding%20Lessons/gpt_api.ipynb#W4sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# Display current frame\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'myMessage' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def openWebcamWindow():\n",
    "\n",
    "    # Set up webcam capture\n",
    "    vid = cv2.VideoCapture(0, cv2.CAP_DSHOW)\n",
    "\n",
    "    frameNumber = 30\n",
    "    emotionList = []\n",
    "    x = 0\n",
    "\n",
    "    blueColor = (0, 0, 255)\n",
    "    redColor = (255, 0, 0)\n",
    "    greenColor = (0, 255, 0)\n",
    "    thickness = 3\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    recordingText = \"\"\n",
    "    global myMessage\n",
    "    global chatGPTMessage\n",
    "    cv2.imshow(\"Facial Detection\", vid.read()[1])\n",
    "    \n",
    "    def mouse_callback(event, x, y, flags, params):\n",
    "        if(event == cv2.EVENT_LBUTTONDOWN and x <= 100 and y <= 100):\n",
    "            global ra\n",
    "            ra = recordAudio()\n",
    "            recordingText = \"RECORDING\"\n",
    "        if(event == cv2.EVENT_LBUTTONDOWN and (x >= 120 and x <= 220) and y <= 100):\n",
    "            print(ra)\n",
    "            rec = True\n",
    "            recordingText = \"\"\n",
    "            chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                                                                                {\"role\": \"user\", \"content\": ra[\"text\"]}])\n",
    "            myMessage = \"Your Message: \" + ra[\"text\"]\n",
    "            chatGPTMessage = \"ChatGPT: \" + chat_completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "            \n",
    "            \n",
    "    # On mouse click, run a method             \\/ this one\n",
    "    cv2.setMouseCallback('Facial Detection', mouse_callback)\n",
    "\n",
    "    while True:\n",
    "        # Get current frame from webcam - vid.read() returns 2 things, _, is used for the things we don't care about\n",
    "        _, frame = vid.read()\n",
    "        x = 0\n",
    "\n",
    "        analytics = DeepFace.analyze(frame, actions = ['emotion'], enforce_detection=False, silent=True)\n",
    "        \n",
    "        for x in range(len(analytics)):\n",
    "            \n",
    "            if(len(emotionList) > 30):\n",
    "                emotionList.pop(len(emotionList)-1) # Removes last emotion from the list\n",
    "\n",
    "            emotionList.insert(0, analytics[x]['dominant_emotion']) # Adds dominant emotion to list\n",
    "            \n",
    "            mainEmotion = \"\" \n",
    "            mostCommonEmotion = max(set(emotionList), key=emotionList.count)\n",
    "            mainEmotion = mostCommonEmotion\n",
    "                \n",
    "            startingPoint = (analytics[x]['region']['x'], analytics[x]['region']['y'])\n",
    "            endingPoint = (analytics[x]['region']['x'] + analytics[x]['region']['w'], analytics[x]['region']['y'] + analytics[x]['region']['h'])\n",
    "            cv2.rectangle(frame, startingPoint, endingPoint, blueColor, thickness)\n",
    "            cv2.putText(frame, mainEmotion, startingPoint, font, 1, redColor, thickness)\n",
    "            cv2.rectangle(frame, (0, 0), (100, 100), greenColor, thickness)\n",
    "            cv2.rectangle(frame, (120, 0), (220, 100), greenColor, thickness)\n",
    "        \n",
    "        cv2.putText(frame, myMessage, (250, 75), font, 1/2, redColor, 2)\n",
    "        cv2.putText(frame, chatGPTMessage, (250, 125), font, 1/2, redColor, 2)\n",
    "            \n",
    "        \n",
    "        # Display current frame\n",
    "        cv2.imshow(\"Facial Detection\", frame)\n",
    "        x += 1\n",
    "        \n",
    "        # Press q to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    vid.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "openWebcamWindow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  6.68it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'emotion': {'angry': 0.001027339264808421, 'disgust': 9.338208714727861e-10, 'fear': 0.0023542822912443117, 'happy': 0.05763810238692775, 'sad': 0.1472810720335586, 'surprise': 0.08147145757840321, 'neutral': 99.71022604168809}, 'dominant_emotion': 'neutral', 'region': {'x': 91, 'y': 58, 'w': 83, 'h': 83}, 'age': 19, 'gender': {'Woman': 0.6382221821695566, 'Man': 99.36177730560303}, 'dominant_gender': 'Man', 'race': {'asian': 6.2889449298381805, 'indian': 3.5093825310468674, 'black': 1.968146301805973, 'white': 51.9415020942688, 'middle eastern': 19.873665273189545, 'latino hispanic': 16.418352723121643}, 'dominant_race': 'white'}]\n",
      "91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def FaceAnalyze(img):\n",
    "    analytics = DeepFace.analyze(img)\n",
    "    print(analytics)\n",
    "    print(analytics[0]['region']['x'])\n",
    "\n",
    "FaceAnalyze('babyface.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 15.90it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 13.02it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 16.15it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 16.80it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 13.95it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 13.24it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 12.90it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 13.53it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 12.85it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 15.65it/s]\n"
     ]
    }
   ],
   "source": [
    "def faceDetection(img):\n",
    "\n",
    "    # Get the image from img\n",
    "    img = cv2.imread(img)\n",
    "    \n",
    "    # Shrink images\n",
    "    img = cv2.resize(img, (0,0), fx=0.5, fy=0.5)\n",
    "\n",
    "    analytics = DeepFace.analyze(img, actions = ['emotion'])\n",
    "\n",
    "    blueColor = (0, 0, 255)\n",
    "    redColor = (255, 0, 0)\n",
    "    thickness = 3\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    for x in range(len(analytics)):\n",
    "        \n",
    "        startingPoint = (analytics[x]['region']['x'], analytics[x]['region']['y'])\n",
    "        endingPoint = (analytics[x]['region']['x'] + analytics[x]['region']['w'], analytics[x]['region']['y'] + analytics[x]['region']['h'])\n",
    "        cv2.rectangle(img, startingPoint, endingPoint, blueColor, thickness)\n",
    "        cv2.putText(img, analytics[x]['dominant_emotion'], startingPoint, font, 1, redColor, thickness)\n",
    "\n",
    "    # Displays the image\n",
    "    cv2.imshow('ffaaccee', img)\n",
    " \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "faceDetection('manyFaces.jpeg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openImage(img):\n",
    "\n",
    "    # Get the image from img\n",
    "    img = cv2.imread(img)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Only include pixels in the range of the color\n",
    "    mask = cv2.inRange(hsv, (22, 250, 250), (24, 260, 260))\n",
    "    inverted_mask = cv2.bitwise_not(mask) #change\n",
    "    \n",
    "    # Apply the mask on hsv\n",
    "    hsv_mask = cv2.bitwise_and(img, img, mask=inverted_mask)#change\n",
    "    \n",
    "    imageWidth = img.shape[0] // 2\n",
    "    imageHeight = img.shape[1] // 2\n",
    "\n",
    "    greenColor = (0, 255, 0)\n",
    "    startingPoint = (imageHeight - 10, imageWidth - 10)\n",
    "    endingPoint = (imageHeight + 10, imageWidth + 10)\n",
    "    thickness = -1\n",
    "\n",
    "    cv2.rectangle(img, startingPoint, endingPoint, greenColor, thickness)\n",
    "\n",
    "    # Displays the image\n",
    "    #cv2.imshow('original image', img)\n",
    "    #cv2.imshow('with mask',hsv_mask)\n",
    "    cv2.imshow(\"Image with green box\", img)\n",
    "    \n",
    "    \n",
    "\n",
    "    def mouse_callback(event, x, y, flags, params):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            print(hsv[y,x])\n",
    "\n",
    "    cv2.setMouseCallback('Image with green box', mouse_callback)\n",
    "\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "openImage('C:/Users/brody/Pictures/Camera Roll/DerpyTree.jpeg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
